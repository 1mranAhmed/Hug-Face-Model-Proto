# -*- coding: utf-8 -*-
"""Hug-Face-Model-Proto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wjn5TjTF43fLw417q0KsB_EQReDpd3y4
"""

! pip install git+https://github.com/huggingface/transformers -q

from transformers import pipeline

"""Speech To Text"""

whisper = pipeline("automatic-speech-recognition", model="openai/whisper-large-v3")

text = whisper('/content/Hello.mp3')

"""Object Detection"""

!pip install accelerate

!pip install flash_attn

from transformers import DetrImageProcessor, DetrForObjectDetection
import torch
from PIL import Image
import requests

def detect_objects_in_image(image_path_or_url):
    if image_path_or_url.startswith("http"):
        image = Image.open(requests.get(image_path_or_url, stream=True).raw)
    else:
        image = Image.open(image_path_or_url)

    processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50", revision="no_timm")
    model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50", revision="no_timm")

    inputs = processor(images=image, return_tensors="pt")
    outputs = model(**inputs)

    target_sizes = torch.tensor([image.size[::-1]])
    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]

    for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
        box = [round(i, 2) for i in box.tolist()]
        print(
                f"Detected {model.config.id2label[label.item()]} with confidence "
                f"{round(score.item(), 3)} at location {box}"
        )

image_path_or_url = input("Enter the path or URL of the image: ")
detect_objects_in_image(image_path_or_url)

"""Image to Text"""

pip install diffusers --upgrade

import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

img_url = 'https://media.istockphoto.com/id/473313240/photo/children-playing-in-the-park-at-playground-and-communicating.jpg?s=612x612&w=0&k=20&c=eMpRprDUvFC35nYizsBKlUPiIU9aDt8v9bKzu-ZQagI='
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

text = "a photography of"
inputs = processor(raw_image, text, return_tensors="pt")

out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))

inputs = processor(raw_image, return_tensors="pt")

out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))

